@article{zaffalon2012ijar,
 abstract = {Predictions made by imprecise-probability models are
often indeterminate (that is, set-valued). Measuring
the quality of an indeterminate prediction by a single
number is important to fairly compare different models,
but a principled approach to this problem is currently
missing. In this paper we derive, from a set of
assumptions, a metric to evaluate the predictions of
credal classifiers. These are supervised learning
models that issue set-valued predictions. The metric
turns out to be made of an objective component, and
another that is related to the decision-makerâ€™s
degree of risk aversion to the variability of
predictions. We discuss when the measure can be
rendered independent of such a degree, and provide
insights as to how the comparison of classifiers based
on the new measure changes with the number of
predictions to be made. Finally, we make extensive
empirical tests of credal, as well as precise,
classifiers by using the new metric. This shows the
practical usefulness of the metric, while yielding a
first insightful and extensive comparison of credal
classifiers.},
 author = {Marco Zaffalon and Giorgio Corani and Denis Deratani
Mauá},
 doi = {10.1016/j.ijar.2012.06.022},
 issn = {0888-613X},
 journal = {International Journal of Approximate Reasoning},
 number = {8},
 pages = {1282--1301},
 selected = {1},
 title = {Evaluating credal classifiers by utility-discounted
predictive accuracy},
 url = {http://www.sciencedirect.com/science/article/pii/S0888613X12000989?v=s5},
 volume = {53},
 year = {2012}
}

