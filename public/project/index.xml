<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | </title>
    <link>http://localhost:1313/~ddm/project/</link>
      <atom:link href="http://localhost:1313/~ddm/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Wed, 01 Feb 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/~ddm/media/icon_hu_645fa481986063ef.png</url>
      <title>Projects</title>
      <link>http://localhost:1313/~ddm/project/</link>
    </image>
    
    <item>
      <title>Neural Inductive Logic Programming</title>
      <link>http://localhost:1313/~ddm/project/neuroproblog/</link>
      <pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/~ddm/project/neuroproblog/</guid>
      <description>&lt;h2 id=&#34;hahahugoshortcode18s0hbhb-funding&#34;&gt;
  &lt;span class=&#34;inline-block  pr-1&#34;&gt;
    &lt;svg style=&#34;height: 1em; transform: translateY(0.1em);&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M2.25 18.75a60.07 60.07 0 0 1 15.797 2.101c.727.198 1.453-.342 1.453-1.096V18.75M3.75 4.5v.75A.75.75 0 0 1 3 6h-.75m0 0v-.375c0-.621.504-1.125 1.125-1.125H20.25M2.25 6v9m18-10.5v.75c0 .414.336.75.75.75h.75m-1.5-1.5h.375c.621 0 1.125.504 1.125 1.125v9.75c0 .621-.504 1.125-1.125 1.125h-.375m1.5-1.5H21a.75.75 0 0 0-.75.75v.75m0 0H3.75m0 0h-.375a1.125 1.125 0 0 1-1.125-1.125V15m1.5 1.5v-.75A.75.75 0 0 0 3 15h-.75M15 10.5a3 3 0 1 1-6 0a3 3 0 0 1 6 0m3 0h.008v.008H18zm-12 0h.008v.008H6z&#34;/&gt;&lt;/svg&gt;
  &lt;/span&gt; Funding&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;FAPESP PI 2022/02937-9 (2023-02-01 to 2028-01-31)&lt;/li&gt;
&lt;li&gt;CNPq Productivity 305136/2022-4 (2023-03-01 to 2026-02-28)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;hahahugoshortcode18s1hbhb-participants&#34;&gt;
  &lt;span class=&#34;inline-block  pr-1&#34;&gt;
    &lt;svg style=&#34;height: 1em; transform: translateY(0.1em);&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M15 19.128a9.38 9.38 0 0 0 2.625.372a9.337 9.337 0 0 0 4.121-.952a4.125 4.125 0 0 0-7.533-2.493M15 19.128v-.003c0-1.113-.285-2.16-.786-3.07M15 19.128v.106A12.318 12.318 0 0 1 8.624 21c-2.331 0-4.512-.645-6.374-1.766l-.001-.109a6.375 6.375 0 0 1 11.964-3.07M12 6.375a3.375 3.375 0 1 1-6.75 0a3.375 3.375 0 0 1 6.75 0m8.25 2.25a2.625 2.625 0 1 1-5.25 0a2.625 2.625 0 0 1 5.25 0&#34;/&gt;&lt;/svg&gt;
  &lt;/span&gt; Participants&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Denis Deratani Mauá, Coordinator&lt;/li&gt;
&lt;li&gt;Fabio G. Cozman, Associated Researcher&lt;/li&gt;
&lt;li&gt;Igor Cataneo Silveira, PhD candidate, Researcher&lt;/li&gt;
&lt;li&gt;Naomi de Moraes, PhD candidate, Researcher&lt;/li&gt;
&lt;li&gt;Renato Lui Geh, Software Developer, Researcher&lt;/li&gt;
&lt;li&gt;Jonas Gonçalves, MSc student, Researcher&lt;/li&gt;
&lt;li&gt;Thiago Casagrande, MSc student, Researcher&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Deep learning techniques have shown impressive results in low-level cognitive task such as image and speech recognition as well as in high-level cognitive tasks such as question answering and stochastic planning. Yet, developing effective deep learning solutions is notoriously challenging as they require massive amounts of data and compute (often beyond limited budgets of typical users), are very sensitive to domain shifts, and produce undesirable output that undermine end-user trust in the system. Good old-fashioned AI techniques based on knowledge representation and symbol manipulation are data efficient, generalizable, and produce mostly verifiable behavior; however they scale poorly, require costly knowledge acquisition procedures and have difficulty in coping with noise and uncertainty that are ubiquitous in real settings. Neurosymbolic approaches have recently re-emerged as a means to take the best of both approaches and deliver systems that are expressive and scalable, yet interpretable, generalizable, data efficient and trustworthy. This document describes a five-year research proposal to study the further development of neurosymbolic approaches based on logic programming, procedural probabilistic programming and combinations of both. Ultimately, this research seeks to advance the state-of-the-art of learning-based agents that go beyond the dominant view of learning as an optimization task in a continuous space guided by input-output examples. To this end, we shall extend current neursymbolic logic programming systems with more expressive constructs and more efficient learning techniques, and evaluate them in challenging cognitive tasks such as text-based question answering and argumentation.&lt;/p&gt;
&lt;h2 id=&#34;outcomes&#34;&gt;Outcomes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;
  &lt;span class=&#34;inline-block  pr-1&#34;&gt;
    &lt;svg style=&#34;height: 1em; transform: translateY(0.1em);&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M12 21a9.004 9.004 0 0 0 8.716-6.747M12 21a9.004 9.004 0 0 1-8.716-6.747M12 21c2.485 0 4.5-4.03 4.5-9S14.485 3 12 3m0 18c-2.485 0-4.5-4.03-4.5-9S9.515 3 12 3m0 0a8.997 8.997 0 0 1 7.843 4.582M12 3a8.997 8.997 0 0 0-7.843 4.582m15.686 0A11.953 11.953 0 0 1 12 10.5c-2.998 0-5.74-1.1-7.843-2.918m15.686 0A8.959 8.959 0 0 1 21 12c0 .778-.099 1.533-.284 2.253m0 0A17.919 17.919 0 0 1 12 16.5a17.92 17.92 0 0 1-8.716-2.247m0 0A9.015 9.015 0 0 1 3 12c0-1.605.42-3.113 1.157-4.418&#34;/&gt;&lt;/svg&gt;
  &lt;/span&gt; The dPASP system: &lt;a href=&#34;https://kamel.ime.usp.br/dpasp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kamel.ime.usp.br/dpasp&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Tutorial: &lt;a href=&#34;https://kamel.ime.usp.br/pages/learn_dpasp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://kamel.ime.usp.br/pages/learn_dpasp&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;klricml2023.pdf&#34;&gt;Overview&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
  &lt;span class=&#34;inline-block  pr-1&#34;&gt;
    &lt;svg style=&#34;height: 1em; transform: translateY(0.1em);&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9&#34;/&gt;&lt;/svg&gt;
  &lt;/span&gt; &lt;a href=&#34;http://localhost:1313/~ddm/publication/silveira-2024-propor/&#34;&gt;PROPOR 2024&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
  &lt;span class=&#34;inline-block  pr-1&#34;&gt;
    &lt;svg style=&#34;height: 1em; transform: translateY(0.1em);&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9&#34;/&gt;&lt;/svg&gt;
  &lt;/span&gt; &lt;a href=&#34;http://localhost:1313/~ddm/publication/rocha-2023-epia/&#34;&gt;EPIA 2023&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
  &lt;span class=&#34;inline-block  pr-1&#34;&gt;
    &lt;svg style=&#34;height: 1em; transform: translateY(0.1em);&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34; viewBox=&#34;0 0 24 24&#34;&gt;&lt;path fill=&#34;none&#34; stroke=&#34;currentColor&#34; stroke-linecap=&#34;round&#34; stroke-linejoin=&#34;round&#34; stroke-width=&#34;1.5&#34; d=&#34;M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9&#34;/&gt;&lt;/svg&gt;
  &lt;/span&gt; &lt;a href=&#34;http://localhost:1313/~ddm/publication/maua-2023-isipta/&#34;&gt;ISIPTA 2023&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Knowledge-Enhanced Machine Learning</title>
      <link>http://localhost:1313/~ddm/project/keml-blab/</link>
      <pubDate>Mon, 18 Apr 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/~ddm/project/keml-blab/</guid>
      <description>&lt;p&gt;The broad goal of this research challenge has been to merge data-driven learning and knowledge-based reasoning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predictive Uncertainty Estimators for Sum-Product Networks</title>
      <link>http://localhost:1313/~ddm/project/pq2020/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/~ddm/project/pq2020/</guid>
      <description>&lt;h2 id=&#34;funding-agency&#34;&gt;Funding Agency&lt;/h2&gt;
&lt;p&gt;CNPq grant 304012/2019-0 (ended)&lt;/p&gt;
&lt;h2 id=&#34;coordinator&#34;&gt;Coordinator&lt;/h2&gt;
&lt;p&gt;Denis Deratani Mauá&lt;/p&gt;
&lt;h2 id=&#34;participants&#34;&gt;Participants&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Denis Deratani Mauá&lt;/li&gt;
&lt;li&gt;Julissa G. Villanueva, PhD candidate at IME-USP&lt;/li&gt;
&lt;li&gt;Renato Lui Geh, MSc student at IME-USP&lt;/li&gt;
&lt;li&gt;Decio L. Soares, MSc student at IME-USP&lt;/li&gt;
&lt;li&gt;Jonas Gonçalves, Undergraduate student at IME-USP&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;abstract&#34;&gt;Abstract&lt;/h2&gt;
&lt;p&gt;Deep models have recently obtained impressive results in a
wide range of tasks and domains, from computer vision to
image and text processing to diagnosis and automated
reasoning. Sum-product networks are special type of neural
networks targeted at the representation of
high-dimensionality probability distributions. Notably, the
structure of a sum-product network (i.e., its graph) encodes
a number of probabilistic independence assertions. In
addition, each node of the network computes a valid
distribution over its part of the variables. Such
probabilistic semantics facilitates debugging, allows a wide
range of queries and data (including missing and noisy data)
to be handled properly and efficiently, and enables
efficient structure learning algorithms.&lt;/p&gt;
&lt;p&gt;In many applications, it is desirable to have not only a
prediction (e.g., the probability of observing a certain
object, say an image), but a confidence measure about its
prediction. As sum-product networks are learned from data,
the probabilities they calculate can be overly confident or
too sensitive to hyperparameters on regions where data was
scarce, conflicting or too noisy. In this research project,
we plan on following two possible approaches to estimating
the uncertainty of a sum-product network prediction. The
first approach estimates uncertainty by analyzing the effect
that small perturbations of the data have on the prediction.
The second approach considers the variability of predictions
among different networks (learned from the same data).&lt;/p&gt;
&lt;p&gt;This research investigates predictive uncertainty estimators for sum-product
networks.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The ENEM Challenge</title>
      <link>http://localhost:1313/~ddm/project/enem/</link>
      <pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/~ddm/project/enem/</guid>
      <description>&lt;p&gt;It is widely accepted that university entrance exams require human-level
intelligence to be satisfactorily solved. In particular, such exams
require skills such as text and image understanding, informational retrieval,
commonsense reasoning, and mathematical thinking.&lt;/p&gt;
&lt;p&gt;Thus, university entrance exams constitute a good test for real Artificial Intelligence techniques.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://en.wikipedia.org/wiki/Exame_Nacional_do_Ensino_M%C3%A9dio&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Exame Nacional do Ensino Médio (ENEM)&lt;/a&gt; is an advanced High-School level exam widely applied every year by the Brazilian government to students that wish to undertake a University degree.&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;ENEM Challenge&lt;/strong&gt; consists in designing an autonomous system that matches the performance of a human students on the exam. The overall goal is to foster and evaluate the development of Artificial Intelligence techniques that have good performance on complex cognitive tasks, not particularly designed for AI systems. In addition, this challenge aims to promote and give more visiblity to the development of NLP tools for Brazilian Portuguese.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The informal and witty aim of the challenge is to get a digital student to be accepted at a main Brazilian university&lt;/strong&gt;.&lt;/p&gt;
&lt;h1 id=&#34;dataset&#34;&gt;Dataset&lt;/h1&gt;
&lt;p&gt;The ENEM exam consists of the writing of an essay and an objective part containing 180 multiple choice questions. The questions are divided into four groups of 45 questions each, namely, Humanities, Languages, Sciences and Mathematics.&lt;/p&gt;
&lt;p&gt;This dataset contains only the textual part of objective part, segmented in questions. Every question is divided into 3 parts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;em&gt;header&lt;/em&gt;, contianing background knwoledge in the form of an image, text or table is given;&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;statement&lt;/em&gt;, containing the text of the question;&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;answers&lt;/em&gt;, split into five fields, each containing the text of a candidate-answer (A,B,C,D,E), along with an additional tag informing the correct answer.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, every question is annotated with the following tags:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;id&lt;/code&gt;: the question number in that assessment;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;image&lt;/code&gt;: does it has an accompanying image in the exam?&lt;/li&gt;
&lt;li&gt;&lt;code&gt;EK&lt;/code&gt;: does it require knowledge not given in the header?&lt;/li&gt;
&lt;li&gt;&lt;code&gt;TC&lt;/code&gt;: does it require textual understanding of the header?&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IC&lt;/code&gt;: does it require understanding of the associated image?&lt;/li&gt;
&lt;li&gt;&lt;code&gt;DS&lt;/code&gt;: does it require complex inference or domain specific knowledge?&lt;/li&gt;
&lt;li&gt;&lt;code&gt;MR&lt;/code&gt;: does it requires transforming instructions in natural language into mathematical formula?&lt;/li&gt;
&lt;li&gt;&lt;code&gt;CE&lt;/code&gt;: does it require treating chemical elements especially?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Currently, the challenge uses an XML version of questions taken from the exams between 2009 and 2017.&lt;/p&gt;
&lt;p&gt;A more in-depth description of the dataset is found &lt;a href=&#34;ENEM-GuidingTest.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h1 id=&#34;download&#34;&gt;Download&lt;/h1&gt;
&lt;p&gt;The 776kb zip archive containing the XML file can be downloaded &lt;a href=&#34;ENEMdataset.zip&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;solutions&#34;&gt;Solutions&lt;/h1&gt;
&lt;p&gt;If you design a new solution, please &lt;a href=&#34;http://localhost:1313/~ddm/#contact&#34;&gt;send me a message&lt;/a&gt; reporting accuracy, reference (if any) and information as below. Here is a list of current solutions:&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Paper&lt;/th&gt;
          &lt;th&gt;Accuracy&lt;/th&gt;
          &lt;th&gt;Note&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;citation&#34;&gt;Citation&lt;/h1&gt;
&lt;p&gt;If you use this dataset, please cite the following work:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    @InProceedings{ ENEM-Challenge,
            author = {Silveira, Igor Cataneo and Mau\&#39;a, Denis Deratani},
            booktitle = {Proceedings of the 6th Brazilian Conference on Intelligent Systems},
            series = {BRACIS},
            title = {University Entrance Exam as a Guiding Test for Artificial Intelligence},
            pages = {426--431},
            year = {2017}
    }
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
